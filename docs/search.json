[
  {
    "objectID": "DiabetesModeling.html",
    "href": "DiabetesModeling.html",
    "title": "DiabetesModeling",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin"
  },
  {
    "objectID": "DiabetesModeling.html#introduction",
    "href": "DiabetesModeling.html#introduction",
    "title": "DiabetesModeling",
    "section": "Introduction",
    "text": "Introduction\nWe will read in the R Data Set that we saved in the previous file\n\ndata &lt;- readRDS(\"data_factors.rds\")\n\n\nSplitting the Data\nWe will split the data in a 70/30 split. So, 70 percent of the data in the training set, and 30 percent in the test set. We’ll use logLoss as our metric to evaluate models. For all model types we will use logLoss with 5 fold cross-validation to select the best model. I will set up my own grid of tuning parameters in any model where that is possible.\n“Log-loss is indicative of how close the prediction probability is to the corresponding actual/true value (0 or 1 in case of binary classification). The more the predicted probability diverges from the actual value, the higher is the log-loss value.” https://towardsdatascience.com/intuition-behind-log-loss-score-4e0c9979680a\nThis is why we want to choose a model with the lowest Log-loss value. Log-loss is useful because instead of calculating a response as correct / incorrect in terms of accuracy, it takes into account how close it is to the true value. So for instance, if the actual response was 1 and the predicted response was 0.9, our accuracy metric would count this as incorrect because it is not equal to the same value.\n\nset.seed(558)\n# Creating an 70/30 split\nsplit &lt;- createDataPartition(y = data$Diabetes_binaryF, p = 0.7, list = FALSE)\ntrain &lt;- data[split, ]\ntest &lt;- data[-split, ]\ndim(train)\n\n[1] 177577     43"
  },
  {
    "objectID": "DiabetesModeling.html#logistic-regression-models",
    "href": "DiabetesModeling.html#logistic-regression-models",
    "title": "DiabetesModeling",
    "section": "Logistic Regression Models",
    "text": "Logistic Regression Models\nA logistic regression model is a regression model where the response we are trying to predict is a binary outcome. The output from this model is generally given as a log odds ratio for each respective parameter. Exponentiating each of these coefficients with base ‘e’, we will get the odds ratio. This tells us how a change in a predictor by one unit affects the odds of the occurrence of the outcome.\n\n# Model 1\nmodel1 &lt;- Diabetes_binaryF ~ HighBPF + HighCholF + SmokerF\n\n# Model 2\nmodel2 &lt;- Diabetes_binaryF ~ SexF + AgeF + EducationF + IncomeF\n\n# Model 3\nmodel3 &lt;- Diabetes_binaryF ~ HvyAlcoholConsumpF + PhysHlthF + MentHlthF + SmokerF + HighCholF + BMI\n\n\nlogFit1 &lt;- train(model1, \n              data = train,\n              method = \"glm\",\n              metric = \"logLoss\",\n              family = \"binomial\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = trainControl(method = \"cv\", \n                                       number = 5,\n                                       classProbs = TRUE,\n                                       summaryFunction = mnLogLoss))\nlogFit2 &lt;- train(model2, \n              data = train,\n              method = \"glm\",\n              metric = \"logLoss\",\n              family = \"binomial\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = trainControl(method = \"cv\", \n                                       number = 5,\n                                       classProbs = TRUE,\n                                       summaryFunction = mnLogLoss))\nlogFit3 &lt;- train(model3, \n              data = train,\n              method = \"glm\",\n              metric = \"logLoss\",\n              family = \"binomial\",\n              preProcess = c(\"center\", \"scale\"),\n              trControl = trainControl(method = \"cv\", \n                                       number = 5,\n                                       classProbs = TRUE,\n                                       summaryFunction = mnLogLoss))\n\n\nlogFit1$results$logLoss\n\n[1] 0.3597528\n\nlogFit2$results$logLoss\n\n[1] 0.3698428\n\nlogFit3$results$logLoss\n\n[1] 0.3570491"
  },
  {
    "objectID": "DiabetesModeling.html#classification-tree",
    "href": "DiabetesModeling.html#classification-tree",
    "title": "DiabetesModeling",
    "section": "Classification Tree",
    "text": "Classification Tree\nA classification tree is a type of decision tree that uses the values of predictors to make a prediction of the classification of the response for that data point. It is very easy to visualize / interpret and is not computationally expensive. Trees also automatically account for interactions. For classification, the prediction is the most common outcome.\nWe will fit a classification tree and test multiple complexity parameters to find the best one, based on the lowest logLoss.\n\nmodel &lt;- Diabetes_binaryF ~  PhysHlthF + MentHlthF + SexF + AgeF + IncomeF + EducationF + BMI \n\n\ntreeFit &lt;- train(model, \n              data = train,\n              method = \"rpart\",\n              metric = \"logLoss\",\n              preProcess = c(\"center\",\"scale\"),\n              trControl = trainControl(method = \"cv\", \n                                       number = 3,\n                                       classProbs = TRUE,\n                                       summaryFunction = mnLogLoss),\n              tuneGrid = expand.grid(cp = seq(from = 0, to = 0.01, by = 0.001)))\ntreeFit\n\nCART \n\n177577 samples\n     7 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nPre-processing: centered (86), scaled (86) \nResampling: Cross-Validated (3 fold) \nSummary of sample sizes: 118385, 118385, 118384 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.000  0.3824793\n  0.001  0.4037576\n  0.002  0.4037576\n  0.003  0.4037576\n  0.004  0.4037576\n  0.005  0.4037576\n  0.006  0.4037576\n  0.007  0.4037576\n  0.008  0.4037576\n  0.009  0.4037576\n  0.010  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.\n\n\nThe best logLoss was found when our cp = 0, giving us a logLoss of roughly 0.38"
  },
  {
    "objectID": "DiabetesModeling.html#random-forest",
    "href": "DiabetesModeling.html#random-forest",
    "title": "DiabetesModeling",
    "section": "Random Forest",
    "text": "Random Forest\nA Random Forest builds hundreds of classification trees and outputs the most common classifications of the individual trees. This is what it uses for the predictions, and it is generally much more accurate. The downside, is that it is way more computationally intensive.\nNow we will run a random forest and find the mtry value with the lowest logLoss.\n\nrfFit &lt;- train(model, \n              data = train,\n              method = \"rf\",\n              metric = \"logLoss\",\n              trControl = trainControl(method = \"cv\", \n                                       number = 2, # should be 5 \n                                       classProbs = TRUE,\n                                       summaryFunction = mnLogLoss),\n              tuneGrid = data.frame(mtry = 1:5),\n              ntree = 50) # this should be bigger, but its 2 am and i cant get it to run at any higher number\nrfFit\n\nRandom Forest \n\n177577 samples\n     7 predictor\n     2 classes: 'No_Diabetes', 'Diabetes' \n\nNo pre-processing\nResampling: Cross-Validated (2 fold) \nSummary of sample sizes: 88789, 88788 \nResampling results across tuning parameters:\n\n  mtry  logLoss \n  1     4.744121\n  2     4.104653\n  3     3.315265\n  4     2.787575\n  5     2.439324\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 5."
  },
  {
    "objectID": "DiabetesModeling.html#final-model-selection",
    "href": "DiabetesModeling.html#final-model-selection",
    "title": "DiabetesModeling",
    "section": "Final Model Selection",
    "text": "Final Model Selection\nNow we will compare our best model for our logistic regression, classification tree, and our random forest model.\n\nconfusionMatrix(data = test$Diabetes_binaryF, \n                reference = predict(logFit3, newdata = test))\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    No_Diabetes Diabetes\n  No_Diabetes       64811      689\n  Diabetes           9994      609\n                                          \n               Accuracy : 0.8596          \n                 95% CI : (0.8571, 0.8621)\n    No Information Rate : 0.9829          \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 0.0742          \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.86640         \n            Specificity : 0.46918         \n         Pos Pred Value : 0.98948         \n         Neg Pred Value : 0.05744         \n             Prevalence : 0.98294         \n         Detection Rate : 0.85162         \n   Detection Prevalence : 0.86068         \n      Balanced Accuracy : 0.66779         \n                                          \n       'Positive' Class : No_Diabetes     \n                                          \n\nconfusionMatrix(data = test$Diabetes_binaryF, \n                reference = predict(treeFit, newdata = test))\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    No_Diabetes Diabetes\n  No_Diabetes       63708     1792\n  Diabetes           9358     1245\n                                        \n               Accuracy : 0.8535        \n                 95% CI : (0.851, 0.856)\n    No Information Rate : 0.9601        \n    P-Value [Acc &gt; NIR] : 1             \n                                        \n                  Kappa : 0.1285        \n                                        \n Mcnemar's Test P-Value : &lt;2e-16        \n                                        \n            Sensitivity : 0.8719        \n            Specificity : 0.4099        \n         Pos Pred Value : 0.9726        \n         Neg Pred Value : 0.1174        \n             Prevalence : 0.9601        \n         Detection Rate : 0.8371        \n   Detection Prevalence : 0.8607        \n      Balanced Accuracy : 0.6409        \n                                        \n       'Positive' Class : No_Diabetes   \n                                        \n\nconfusionMatrix(data = test$Diabetes_binaryF, \n                reference = predict(rfFit, newdata = test))\n\nConfusion Matrix and Statistics\n\n             Reference\nPrediction    No_Diabetes Diabetes\n  No_Diabetes       65500        0\n  Diabetes          10602        1\n                                          \n               Accuracy : 0.8607          \n                 95% CI : (0.8582, 0.8631)\n    No Information Rate : 1               \n    P-Value [Acc &gt; NIR] : 1               \n                                          \n                  Kappa : 2e-04           \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 8.607e-01       \n            Specificity : 1.000e+00       \n         Pos Pred Value : 1.000e+00       \n         Neg Pred Value : 9.431e-05       \n             Prevalence : 1.000e+00       \n         Detection Rate : 8.607e-01       \n   Detection Prevalence : 8.607e-01       \n      Balanced Accuracy : 9.303e-01       \n                                          \n       'Positive' Class : No_Diabetes     \n                                          \n\n\nAs we see, our logistic regression model had a pretty good accuracy and also the best logloss, so we will continue with this model through the API portion, allowing the user to investigate the different predictors."
  },
  {
    "objectID": "DiabetesDataEDA.html",
    "href": "DiabetesDataEDA.html",
    "title": "DiabetesDataEDA",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "DiabetesDataEDA.html#introduction",
    "href": "DiabetesDataEDA.html#introduction",
    "title": "DiabetesDataEDA",
    "section": "Introduction",
    "text": "Introduction\nThe data we will be examining and modeling in this project is a data set with 253,680 survey responses to the CDC’s BRFSS2015. The response variable we will be modeling is a binary variable of whether or not the observation had or did not have diabetes. Some predictors we will examine are:\n\nHighBP (0 = no high BP / 1 = high BP)\nHighChol (0 = no high cholesterol / 1 = high cholesterol)\nSmoker (Have you smoked at least 100 cigarettes in your entire life? 0 = no / 1 = yes)\nSex (0 = female / 1 = male)\nAge (13-level age category (AGEG5YR see codebook))\nEducation (Education level (EDUCA see codebook))\nIncome (Income scale)\nHvyAlcoholConsump (Is or is not a heavy drinker)\nPhysHlth (How many days during the past 30 were you not physically active)\nMentHlth (How many days during the past 30 were you not mentally healthy)\nBMI (Body Mass Index)\n\n\nPurpose of EDA and Modeling Goals\nOur EDA gives us insight to our data and helps guide the modeling that we will be conducting. This can be accessed via the link at the bottom of the page. The purpose of our modeling will be trying to generate the best possible model at predicting whether or not someone has diabetes, based on the values of their predictors. We will be using predictive analysis with a training and test set via the caret package."
  },
  {
    "objectID": "DiabetesDataEDA.html#data",
    "href": "DiabetesDataEDA.html#data",
    "title": "DiabetesDataEDA",
    "section": "Data",
    "text": "Data\n\nReading in Data\n\ndata &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nFactor Conversions\nWe will now convert the necessary columns to factors, specifying appropiate labels for each.\n\ndata_factors &lt;- data |&gt;\n  mutate(\n    Diabetes_binaryF = factor(Diabetes_binary, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"No_Diabetes\", \"Diabetes\")),\n    HighBPF = factor(HighBP, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"No High Blood Pressure\", \"High Blood Pressure\")),\n    HighCholF = factor(HighChol, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"No High Cholesterol\", \"High Cholesterol\")),\n    CholCheckF = factor(CholCheck, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"No Cholesterol Check in 5 Years\", \n                                         \"Yes Cholesterol Check in 5 Years\")),\n    SmokerF = factor(Smoker, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Has not smoked at least 100 cigarettes\", \n                                         \"Has smoked at least 100 cigarettes\")),\n    StrokeF = factor(Stroke, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Has had stroke\", \n                                         \"Has not had stroke\")),\n    HeartDiseaseorAttackF = factor(HeartDiseaseorAttack, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Has not had coronary heart disease or myocardial infarction\", \n                                         \"Has had coronary heart disease or myocardial infarction\")),\n    PhysActivityF = factor(PhysActivity, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Was not physically active within past 30 days\")), \n    FruitsF = factor(Fruits, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Does not consume fruit 1 or more times per day\", \n                                         \"Does consume fruit 1 or more times per day\")),\n    VeggiesF = factor(Veggies, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Does not consume vegetables 1 or more times per day\", \n                                         \"Does consume vegetables 1 or more times per day\")),\n    HvyAlcoholConsumpF = factor(HvyAlcoholConsump, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Is a heavy drinker\", \n                                         \"Is not a heavy drinker\")),\n    AnyHealthcareF = factor(AnyHealthcare, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Has healthcare\", \n                                         \"Does not have healthcare\")),\n    NoDocbcCostF = factor(NoDocbcCost, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Could afford doctor within past year\", \n                                         \"Could not afford doctor within past year\")),\n    GenHlthF = factor(GenHlth, \n                              levels = c(\"1\",\"2\",\"3\",\"4\",\"5\"), \n                              labels = c(\"Excellent\",\n                                         \"Very Good\",\n                                         \"Good\",\n                                         \"Fair\",\n                                         \"Poor\")),\n    MentHlthF = factor(MentHlth,\n                      levels = c(\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\n                                 \"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\n                                 \"26\",\"27\",\"28\",\"29\",\"30\"),\n                      labels = c(\"0 Days\", \"1 Day\", \"2 Days\", \"3 Days\", \"4 Days\", \"5 Days\", \"6 Days\",\n                                 \"7 Days\", \"8 Days\", \"9 Days\", \"10 Days\", \"11 Days\", \n                                 \"12 Days\", \"13 Days\", \"14 Days\", \"15 Days\", \"16 Days\", \n                                 \"17 Days\", \"18 Days\", \"19 Days\", \"20 Days\", \"21 Days\", \n                                 \"22 Days\", \"23 Days\", \"24 Days\", \"25 Days\", \"26 Days\", \n                                 \"27 Days\", \"28 Days\", \"29 Days\", \"30 Days\")),\n    PhysHlthF = factor(PhysHlth,\n                      levels = c(\"0\", \"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\n                                 \"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\n                                 \"26\",\"27\",\"28\",\"29\",\"30\"),\n                      labels = c(\"0 Days\",\"1 Day\", \"2 Days\", \"3 Days\", \"4 Days\", \"5 Days\", \"6 Days\",\n                                 \"7 Days\", \"8 Days\", \"9 Days\", \"10 Days\", \"11 Days\", \n                                 \"12 Days\", \"13 Days\", \"14 Days\", \"15 Days\", \"16 Days\", \n                                 \"17 Days\", \"18 Days\", \"19 Days\", \"20 Days\", \"21 Days\", \n                                 \"22 Days\", \"23 Days\", \"24 Days\", \"25 Days\", \"26 Days\", \n                                 \"27 Days\", \"28 Days\", \"29 Days\", \"30 Days\")),\n    DiffWalkF = factor(DiffWalk, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Does not have difficulty walking or climbing stairs\", \n                                         \"Does have difficulty walking or climbing stairs\")),\n    SexF = factor(Sex, \n                              levels = c(\"0\",\"1\"), \n                              labels = c(\"Female\", \n                                         \"Male\")),\n    AgeF = factor(Age, \n                              levels = c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n                                         \"11\",\"12\",\"13\"), \n                              labels = c(\"Aged 18-24\", \"Aged 25-29\", \"Aged 30-34\",\n                                         \"Aged 35-39\", \"Aged 40-44\", \"Aged 45-49\",\n                                         \"Aged 50-54\", \"Aged 55-59\", \"Aged 60-64\",\n                                         \"Aged 65-69\", \"Aged 70-74\", \"Aged 75-79\",\n                                         \"Aged 80 or older\")),\n    EducationF = factor(Education, \n                              levels = c(\"1\",\"2\",\"3\",\"4\",\"5\", \"6\"), \n                              labels = c(\"Never attended school or only kindergarten\",\n                                         \"Grades 1 through 8 (Elementary)\",\n                                         \"Grades 9 through 11 (Some high school)\",\n                                         \"Grade 12 or GED (High school graduate)\",\n                                         \"College 1 year to 3 years (Some college or technical school)\",\n                                         \"College 4 years or more (College graduate)\")),    \n    \n    \n    IncomeF = factor(Income, \n                              levels = c(\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"), \n                              labels = c(\"Less than $10,000\",\n                                         \"Less than $15,000\",\n                                         \"Less than $20,000\",\n                                         \"Less than $25,000\",\n                                         \"Less than $35,000\",\n                                         \"Less than $50,000\",\n                                         \"Less than $75,000\",\n                                         \"More than $75,000\")))\n\n\n# saving as an RDS to access in the modeling file to not have to repeat the factor changes\nsaveRDS(data_factors, \"data_factors.rds\")\n\n\n\nMissingness\n\ncolSums(is.na(data_factors))\n\n      Diabetes_binary                HighBP              HighChol \n                    0                     0                     0 \n            CholCheck                   BMI                Smoker \n                    0                     0                     0 \n               Stroke  HeartDiseaseorAttack          PhysActivity \n                    0                     0                     0 \n               Fruits               Veggies     HvyAlcoholConsump \n                    0                     0                     0 \n        AnyHealthcare           NoDocbcCost               GenHlth \n                    0                     0                     0 \n             MentHlth              PhysHlth              DiffWalk \n                    0                     0                     0 \n                  Sex                   Age             Education \n                    0                     0                     0 \n               Income      Diabetes_binaryF               HighBPF \n                    0                     0                     0 \n            HighCholF            CholCheckF               SmokerF \n                    0                     0                     0 \n              StrokeF HeartDiseaseorAttackF         PhysActivityF \n                    0                     0                     0 \n              FruitsF              VeggiesF    HvyAlcoholConsumpF \n                    0                     0                     0 \n       AnyHealthcareF          NoDocbcCostF              GenHlthF \n                    0                     0                     0 \n            MentHlthF             PhysHlthF             DiffWalkF \n                    0                     0                     0 \n                 SexF                  AgeF            EducationF \n                    0                     0                     0 \n              IncomeF \n                    0 \n\n\nAs we can see, there are no missing entries within every column."
  },
  {
    "objectID": "DiabetesDataEDA.html#summarizations",
    "href": "DiabetesDataEDA.html#summarizations",
    "title": "DiabetesDataEDA",
    "section": "Summarizations",
    "text": "Summarizations\nLet’s now create some graphs and tables to perform EDA.\n\nggplot(data = data_factors, aes(x = SexF, fill = SexF)) +\n  geom_bar() +\n  facet_grid(~ Diabetes_binaryF)\n\n\n\n\n\n\n\n\n\nconting_data &lt;- as.data.frame(table(data_factors$HighBPF, data_factors$HighCholF, data_factors$Diabetes_binaryF))\ncolnames(conting_data) &lt;- c(\"BloodPressure\", \"Cholesterol\", \"Diabetes\", \"Total\")\n\n\nggplot(data = conting_data, aes(x = BloodPressure, y = Total, fill = Cholesterol)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_grid(~ Diabetes) +\n  theme(axis.text.x = element_text(angle = 10))\n\n\n\n\n\n\n\n\nThis shows us that the vast majority of the observations are those without diabetes. Of those with diabetes, the majority have high blood pressure and those with high blood pressure are more likely to have high cholesterol as well.\n\ntable(data_factors$EducationF, data_factors$IncomeF)\n\n                                                              \n                                                               Less than $10,000\n  Never attended school or only kindergarten                                  37\n  Grades 1 through 8 (Elementary)                                            900\n  Grades 9 through 11 (Some high school)                                    1536\n  Grade 12 or GED (High school graduate)                                    3594\n  College 1 year to 3 years (Some college or technical school)              2437\n  College 4 years or more (College graduate)                                1307\n                                                              \n                                                               Less than $15,000\n  Never attended school or only kindergarten                                  25\n  Grades 1 through 8 (Elementary)                                            741\n  Grades 9 through 11 (Some high school)                                    1465\n  Grade 12 or GED (High school graduate)                                    4692\n  College 1 year to 3 years (Some college or technical school)              3315\n  College 4 years or more (College graduate)                                1545\n                                                              \n                                                               Less than $20,000\n  Never attended school or only kindergarten                                  28\n  Grades 1 through 8 (Elementary)                                            740\n  Grades 9 through 11 (Some high school)                                    1709\n  Grade 12 or GED (High school graduate)                                    6511\n  College 1 year to 3 years (Some college or technical school)              4664\n  College 4 years or more (College graduate)                                2342\n                                                              \n                                                               Less than $25,000\n  Never attended school or only kindergarten                                  18\n  Grades 1 through 8 (Elementary)                                            605\n  Grades 9 through 11 (Some high school)                                    1453\n  Grade 12 or GED (High school graduate)                                    8029\n  College 1 year to 3 years (Some college or technical school)              6310\n  College 4 years or more (College graduate)                                3720\n                                                              \n                                                               Less than $35,000\n  Never attended school or only kindergarten                                  22\n  Grades 1 through 8 (Elementary)                                            478\n  Grades 9 through 11 (Some high school)                                    1268\n  Grade 12 or GED (High school graduate)                                    9046\n  College 1 year to 3 years (Some college or technical school)              8579\n  College 4 years or more (College graduate)                                6490\n                                                              \n                                                               Less than $50,000\n  Never attended school or only kindergarten                                  18\n  Grades 1 through 8 (Elementary)                                            293\n  Grades 9 through 11 (Some high school)                                     921\n  Grade 12 or GED (High school graduate)                                   10872\n  College 1 year to 3 years (Some college or technical school)             11996\n  College 4 years or more (College graduate)                               12370\n                                                              \n                                                               Less than $75,000\n  Never attended school or only kindergarten                                  13\n  Grades 1 through 8 (Elementary)                                            144\n  Grades 9 through 11 (Some high school)                                     590\n  Grade 12 or GED (High school graduate)                                    9492\n  College 1 year to 3 years (Some college or technical school)             13248\n  College 4 years or more (College graduate)                               19732\n                                                              \n                                                               More than $75,000\n  Never attended school or only kindergarten                                  13\n  Grades 1 through 8 (Elementary)                                            142\n  Grades 9 through 11 (Some high school)                                     536\n  Grade 12 or GED (High school graduate)                                   10514\n  College 1 year to 3 years (Some college or technical school)             19361\n  College 4 years or more (College graduate)                               59819\n\n\n\nggplot(data = data_factors, aes(x = AgeF, fill = Diabetes_binaryF)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45)) +\n  facet_grid(~ HvyAlcoholConsumpF)\n\n\n\n\n\n\n\n\nA lot of drinkers in our data set!\nTrying out proportion tables\n\nround(prop.table(table(data_factors$SmokerF, data_factors$Diabetes_binaryF)),2)\n\n                                        \n                                         No_Diabetes Diabetes\n  Has not smoked at least 100 cigarettes        0.49     0.07\n  Has smoked at least 100 cigarettes            0.37     0.07\n\nround(prop.table(table(data_factors$CholCheckF, data_factors$Diabetes_binaryF)),2)\n\n                                  \n                                   No_Diabetes Diabetes\n  No Cholesterol Check in 5 Years         0.04     0.00\n  Yes Cholesterol Check in 5 Years        0.82     0.14\n\nround(prop.table(table(data_factors$StrokeF, data_factors$Diabetes_binaryF)),2)\n\n                    \n                     No_Diabetes Diabetes\n  Has had stroke            0.83     0.13\n  Has not had stroke        0.03     0.01"
  },
  {
    "objectID": "DiabetesDataEDA.html#modeling-html-link",
    "href": "DiabetesDataEDA.html#modeling-html-link",
    "title": "DiabetesDataEDA",
    "section": "Modeling HTML Link",
    "text": "Modeling HTML Link\nLink to Modeing HTML Page"
  }
]